import os, sysimport numpy as npimport pandas as pdfrom tqdm import tqdmfrom gensim.models import Word2Vecfrom gensim.models.callbacks import CallbackAny2Vectqdm.pandas()# PYTHONHASHSEED value is set to 0 to control hash randomization for full reproducibilityhashseed = os.getenv('PYTHONHASHSEED')if not hashseed:  os.environ['PYTHONHASHSEED'] = '0'  os.execv(sys.executable, [sys.executable] + sys.argv)# Create a epoch logger to keep track of training progress class EpochLogger(CallbackAny2Vec):  def __init__(self):    self.epoch = 0  def on_epoch_begin(self, model):    print("Epoch #{} start".format(self.epoch))  def on_epoch_end(self, model):    print("Epoch #{} end".format(self.epoch))    self.epoch += 1epoch_logger = EpochLogger()# Import merged COCA as a pandas dataframecurrent_path = os.getcwd()os.chdir(os.path.join(os.path.abspath('..'), 'corpus'))corpus = pd.read_pickle('preprocessed_coca.pkl')# Train a word embedding model using Word2Vec# For full reproducibility, the model must be limited to a single worker thread (workers = 1)model = Word2Vec(corpus.text,  vector_size = 300,  epochs = 10,   window = 6,   min_count = 30,   workers = 1,  sg = 1,   negative = 10,   seed = 1048596,   callbacks=[epoch_logger])# Save the embedding model in the embedding folderos.chdir(current_path)model.save('coca.model')# Perform simple query task to evaluate the model# The query_term we use for the task is 'african_americans'# We are interested in the top five terms that are most similar to the query termword_vectors = model.wvquery_term = 'african_americans'results = word_vectors.most_similar(query_term, topn = 5)# Print those termsfor result in results:   print(result)